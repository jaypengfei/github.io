---
layout: post
title:  EM算法
date:   2017-08-1 21:37:00 +0800
categories: 机器学习
tag: 机器学习
---

* content
{:toc}


最近在看到机器学习中关于贝叶斯分类器的相关内容，其中介绍了EM算法，但是并没有很好的理解其中的原理。正好手头有一本《统计学习方法》，里面有完整的一章介绍了EM算法，讲解较为细致。所以我就把相关的内容整理 了一下，写到这里。
<hr>
以下内容参考《李航：统计学习方法》
<hr>
&emsp;&emsp;概率模型有时既含有观测变量，又含有隐变量。如果概率模型的变量都是观测变量，那么给定数据。可以直接使用极大似然估计法，或贝叶斯估计法估计模型参数。但是当含有隐变量时，就不能简单地使用这些估计方法。EM算法就是含有隐变量的概率模型参数的极大似然估计法，或极大后验概率估计法。<br>

一个例子
====================================

首先介绍一个使用EM算法的例子：<br>
假设有3枚硬币，分别记做A，B，C。这些硬币正面出现的概率分别是π，p和q。进行如下掷硬币实验：先掷硬币A根据其正面向上与否选出硬币B或硬币C,然后掷选出的硬币，记录掷硬币的结果，正面为1，反面为0；独立的重复n次实验（本次实验n=10），观测结果如下：<br>
1,1,0,1,0,0,1,0,1,1
假设我们只能看到掷硬币的结果，不能观测掷硬币的过程（即不知道最终记录的结果来自硬币B还是C）。那么如何估计三枚硬币正面出现的概率，即三硬币的模型参数？
下面给出求解过程：<br>
三硬币模型可以写作：<br>
![模型定义]({{ '/styles/images/em/em_1.png' | prepend: site.baseurl }})

其中，随机变量y是观测变量（可观测到），表示一次实验的结果是1或0；随机变量z是隐变量（无法观测到），表示为观测到的掷硬币A的结果；θ=(π,p,q)是模型参数。这一模型是以上数据的生成模型。<br>

<hr>
​最后的最后，老婆我爱你。








