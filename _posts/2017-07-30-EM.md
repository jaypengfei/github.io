---
layout: post
title:  EM算法
date:   2017-08-1 21:37:00 +0800
categories: 机器学习
tag: 机器学习
---

* content
{:toc}


&emsp;&emsp;最近在看到机器学习中关于贝叶斯分类器的相关内容，其中介绍了EM算法，但是并没有很好的理解其中的原理。正好手头有一本《统计学习方法》，里面有完整的一章介绍了EM算法，讲解较为细致。所以我就把相关的内容整理 了一下，写到这里。
<hr>
&emsp;&emsp;以下内容参考《李航：统计学习方法》
<hr>
&emsp;&emsp;概率模型有时既含有观测变量，又含有隐变量。如果概率模型的变量都是观测变量，那么给定数据。可以直接使用极大似然估计法，或贝叶斯估计法估计模型参数。但是当含有隐变量时，就不能简单地使用这些估计方法。EM算法就是含有隐变量的概率模型参数的极大似然估计法，或极大后验概率估计法。<br>

一个例子
====================================

&emsp;&emsp;首先介绍一个使用EM算法的例子：<br><br>
&emsp;&emsp;假设有3枚硬币，分别记做A，B，C。这些硬币正面出现的概率分别是π，p和q。进行如下掷硬币实验：先掷硬币A根据其正面向上与否选出硬币B或硬币C,然后掷选出的硬币，记录掷硬币的结果，正面为1，反面为0；独立的重复n次实验（本次实验n=10），观测结果如下：<br><br>
&emsp;&emsp;1,1,0,1,0,0,1,0,1,1<br><br>
&emsp;&emsp;假设我们只能看到掷硬币的结果，不能观测掷硬币的过程（即不知道最终记录的结果来自硬币B还是C）。那么如何估计三枚硬币正面出现的概率，即三硬币的模型参数？
&emsp;&emsp;下面给出求解过程：<br>
&emsp;&emsp;三硬币模型可以写作：<br>

![模型定义]({{ '/styles/images/em/em1.png' | prepend: site.baseurl }})

&emsp;&emsp;其中，随机变量y是观测变量（可观测到），表示一次实验的结果是1或0；随机变量z是隐变量（无法观测到），表示为观测到的掷硬币A的结果；θ=(π,p,q)是模型参数。这一模型是以上数据的生成模型。<br>
&emsp;&emsp;将观测到的数据表示为：<br>

![em]({{ '/styles/images/em/em2.png' | prepend: site.baseurl }})

&emsp;&emsp;未观测到的数据表示为：<br>

![em]({{ '/styles/images/em/em3.png' | prepend: site.baseurl }})

&emsp;&emsp;则观测数据的似然函数为：<br>

![em]({{ '/styles/images/em/em4.png' | prepend: site.baseurl }})

&emsp;&emsp;考虑求模型参数θ=(π,p,q)的极大似然估计，即：<br>

![em]({{ '/styles/images/em/em5.png' | prepend: site.baseurl }})

&emsp;&emsp;这个问题没有解析解，只有通过迭代的方法求解。EM算法就是可以用于求解这个问题的一种迭代算法。下面给出针对上述问题的EM算法：<br>

![em]({{ '/styles/images/em/em6.png' | prepend: site.baseurl }})


<hr>
​最后的最后，老婆我爱你。








