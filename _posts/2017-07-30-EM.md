---
layout: post
title:  EM算法
date:   2017-08-1 21:37:00 +0800
categories: 机器学习
tag: 机器学习
---

* content
{:toc}




&emsp;&emsp;EM算法是一种迭代算法，用于含有隐变量的概率模型参数的极大似然估计，或极大后验概率估计。EM算法的每次迭代由两步组成：E步，求期望；M步求极大。所以这一算法称为`期望极大法`（expectation maximization），简称EM算法。<br>
&emsp;&emsp;以上内容来源于《李航：统计学习方法》。

&emsp;&emsp;最近在看关于机器学习中关于贝叶斯分类器的相关内容，其中介绍了EM算法，但是并没有很好的理解其中的原理。正好在网上查阅相关资料时看到了一篇关于EM的论文《What is the expectation maximization algorithm?》。其中举了一个非常通俗易懂的例子来说明了EM算法是如何在估计隐变量中起作用的。例子如下：

一个例子
====================================

&emsp;&emsp;现在有两枚硬币A和B，我们需要估计的参数是它们各自抛落地面时正面向上的概率。观察的过程是：<br>
+ 随机的挑选A或B，然后连续扔10次，并记录
+ 以上过程重复5次。<br>

&emsp;&emsp;如果我们事先知道每次选择的是A还是B（图.a），那么可以直接进行估计出相应的参数。但是如果我们不知道每次选择的是A还是B（隐变量），只观测到5次循环共50次的抛硬币的结果，这时就无法直接估计A和B正面向上的概率。此时就需要使用EM算法（图.b）。最终估计出我们需要的参数。

![EM算法]({{ '/styles/images/em/em1.png' | prepend: site.baseurl }})
![EM算法]({{ '/styles/images/em/em2.png' | prepend: site.baseurl }})

&emsp;&emsp;现在对图b里的过程进行一些解释：
&emsp;&emsp;+ 1.首先是得到了观察结果（50个）
&emsp;&emsp;+ 2.执行E步，得到每组实验结果来自A或B的概率（如第1行的0.45和0.55）
&emsp;&emsp;+ 3.根据2中计算得到的概率，得到了新的”观察“结果，然后重新计算参数，并将参数进行回代
&emsp;&emsp;+ 4.不断的迭代，直到达到预设的迭代轮数或者是足够的精度。返回最终的结果。


<hr>
​最后的最后，老婆我爱你。








