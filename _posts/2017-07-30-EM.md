---
layout: post
title:  EM算法
date:   2017-08-1 21:37:00 +0800
categories: 机器学习
tag: 机器学习
---

* content
{:toc}




&emsp;&emsp;EM算法是一种迭代算法，用于含有隐变量的概率模型参数的极大似然估计，或极大后验概率估计。EM算法的每次迭代由两步组成：E步，求期望；M步求极大。所以这一算法称为期望极大法（expectation maximization），简称E算法。<br>
&emsp;&emsp;以上内容来源于《李航：统计学习方法》。

&emsp;&emsp;最近在看关于机器学习中关于贝叶斯分类器的相关内容，其中介绍了EM算法，但是并没有很好的理解其中的原理。正好在网上查阅相关资料时看到了一篇关于EM的论文《What is the expectation maximization algorithm?》。其中举了一个非常通俗易懂的例子来说明了EM算法是如何在估计隐变量中起作用的。例子如下：

一个例子
====================================
&emsp;&emsp;现在有两枚硬币A和B，我们需要估计的参数是它们各自抛落地面时正面向上的概率。观察的过程是先随机的挑选A或B，然后连续扔10次，并记录。以上过程重复5次。如果我们事先知道每次选择的是A还是B（图.a），那么可以直接进行估计出相应的参数。但是如果我们不知道每次选择的是A还是B（隐变量），只观测到5次循环共50次的抛硬币的结果，这时就无法直接估计A和B正面向上的概率。此时就需要使用EM算法（图.b）。最终估计出我们需要的参数。

![EM算法]({{ '/styles/images/em/em1.png' | prepend: site.baseurl }})
![EM算法]({{ '/styles/images/em/em2.png' | prepend: site.baseurl }})

&emsp;&emsp;其中，在图b中的E-step(2)中，的参数（0.45-0.65）是：在该种观测结果下，是硬币A的概率（1-P(A)=P(B)）。

<hr>
<hr>
​最后的最后，老婆我爱你。








