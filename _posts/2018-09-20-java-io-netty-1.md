---
layout: post
title:  Netty入门篇（1）之I/O基础
date:   2018-09-20 14:22:00 +0800
categories: Java
tag: netty
---

* content
{:toc}
之前有研究过一些NIO的内容，但是并没有深入的了解。所以现在希望可以系统的学习一下相关的知识。从最基本的I/O到NIO一直到Netty。Netty是由[JBOSS](https://baike.baidu.com/item/JBOSS)提供的一个java开源框架。Netty提供异步的、事件驱动的网络应用程序框架和工具，用以快速开发高性能、高可靠性的网络服务器和客户端程序。

1. I/O基础入门
------------------------------------

Java1.4之前的早期版本对I/O的支持并不完善，开发人员在开发高性能I/O程序时，会面临一些巨大的挑战和困难，主要问题如下：

- 没有数据缓冲区，I/O性能存在问题；
- 没有C/C++的channel的概念，只有输入输出流；
- 同步阻塞式I/.O通信（BIO），通常会导致通信线程被长时间阻塞；
- 支持的字符集有限，硬件可移植性不好。

### 1.1 linux网络I/O模型

Linux内核将所有的外部设备都看做一个文件来操作，对一个文件的读写操作通常会内核提供的系统命令，返回一个file descriptor（fd，文件描述符）。而对一个socket的读写也会有相应的描述符，称为socketfd。描述符就是一个数字，它指向内核中的一个结构体（文件路径，数据取等一些属性）。

根据Unix网络编程对I/O模型的分类，共分为以下5种：

1. 阻塞I/O模型：最常用的I/O模型就是阻塞I/O模型，缺省情况下，所有文件操作都是阻塞的。我们以套接字接口为例来讲解此模型：在进程空间中调用recvfrom，其系统调用直到数据包到达且被复制到应用程序的缓冲区中或者中间发生错误时才返回，在此期间一直会等待，进程在从调用recvfrom开始到它返回的整段时间内都是被阻塞的，因此被称为阻塞I/O模型。

2. 非阻塞I/O模型：recvfrom从应用层到内核的时候，如果该缓冲区没有数的话，就直接返回一个EWOULDBLOCK错误，一般都是对非阻塞I/O模型进行轮询检查这个状态，看内核是不是有数据到来。

3. I/.O复用模型：linux提供select/poll，进程通过将一个或多个fd传递给select或poll系统调用，阻塞在select操作上，这样select/poll可以帮助我们侦测多个fd是否处于继续状态。select/poll是顺序扫描fd是否就绪，而且支持的fd数量有限。因此他的使用受到了一些制约。Linux还提供了一个人epoll系统调用，epoll使用基于时间驱动方式代替顺序扫描，因此性能更高。当有fd就绪时，立即回调函数rollback。

4. 信号驱动I/O模型：首先开启套接字接口信号驱动I/O功能，并通过系统调用sigaction执行一个信号处理函数（此系统调用立即返回，进程继续工作，非阻塞）。当数据准备就绪时，就为该进程生成一个SIGIO信号，通过信号回调通知应用程序调用recvfrom来读取数据，并通知主循环函数处理数据。

5. 异步I/O：告知内核启动某个操作，并让内核在整个操作完成以后（包括将数据复制到用户自己的缓冲区）通知我们。这种模型与信号驱动模型的主要区别是：信号驱动I/O由内核通知我们何时可以开始一个I/O操作；异步I/O模型通知我们操作何时已经完成。

   ​

### 1.2 I/O多路复用技术

在I/O编程过程中,当需要同时处理多个客户端接入请求时, 可以利用多线程或者I/O多路复用技术进行处理.I/O多路复用技术通过把多个I/O的阻塞复用到同一个select的阻塞上,从而使得系统在单线程的情况下可以同时处理多个客户端请求.与传统的多线程模型相比,其最大的优势是系统开销小,系统不需要创建新的额外进程或者线程,更不需要维护这些进程和线程的运行,降低了系统的维护工作,节省了系统资源,I/O多路复用的主要场景如下:

* 服务器需要同时处理多个处于监听状态或者多个连接状态的套接字
* 服务器需要同时处理多种网络协议的套接字

目前支持I/O多路复用的系统调用有select,pselect,poll,epoll.在Linux网络编程中,很长一段时间都适用select做轮询和网络事件通知,然而select的一些固有缺陷导致了它的应用受到了很大的限制.最终Linux不得不再新的内核版本中寻找select的替代方案,最终选择了epoll.epoll与select的原理比较类似,为了克服select的缺点,epoll做了很多重大的改进,如下:

1. 支持一个进程打开的socket描述符(FD)不受限制(仅受限于操作系统的最大文件句柄数)

select的最大缺陷是单个进程所打开的FD是有限的,由FD_SETSIZE设置,默认是1024.这对于那些需要支持上万个TCP连接的大型服务器来说显然太少了.当然可以选择修改这个值然后重新编译内核,不过这会带来网络效率的下降.也可以选择多进程的方案解决这个问题.但是进程间的数据交换非常麻烦.尤其是对于Java来说,由于没有共享内存,需要通过socket通信或者其他方式进行数据同步,这带来了额外的性能损耗,增加了程序复杂度,所以也不是一种完美的解决方案.但是epoll没有这个限制,它所支持的FD的上限是操作系统的最大文件句柄数,远远大于1024.例如1GB的机器,大约是10万个文件句柄.

2. I/O效率不会随着FD数目的增加而线性下降

传统的select/poll的另一个致命缺点是,就是当你拥有一个很大的socket集合时,由于网络延迟或者链路空闲,任何时刻只有少部分的socket是活跃的,但是select/poll每次调用都会线性扫描全部的集合,导致效率呈现线性下降.epoll不存在这个问题,只会对活跃的socket进行操作.这是因为在内核实现中,epoll是根据每个fd上面的callback函数实现的.那么只有活跃的sockeye才会去主动调用callback函数,相当于实现了一个伪AIO.

3. 使用mmap加速内核与用户空间的消息传递

无论是select、poll还是epoll都需要内核把FD消息通知给用户空间,如何避免不必要的内存复制就显得非常重要,epoll是通过内核和用户空间mmap同一块内存来实现的.

4. epoll的API更简单

值得说明的是,用来克服select/poll缺点的方法不只有epoll,epoll只是一种Linux的实现方案.其他还有freeBSD的kqueue,以及更加古老的dev/poll.使用难度递增.

从BIO到NIO是Java通信类库迈出的一小步,但是却对Java在高性能通信领域的发展起到了关键性的推到作用.随着基于NIO的各类框架的发展,以及基于NIO的web服务器的发展,Java在很多领域取代了c++成为企业服务端应用开发的首选语言.

## 2. Select, poll, epoll

select, poll, epoll都是I/O多路复用的机制.I/O多路复用就是通过一种机制,可以监视多个描述符,一旦某个描述符就绪(读或写),能够通知程序进行想的读写操作.但是三者本质上都是同步I/O,因为**都需要在读写时间就绪后自己负责读写,也就是说这个读写过程是阻塞的**而异步I/O则不需要自己负责读写,异步I/O的实现会负责把数据从内核拷贝到用户空间.

* select的实现

调用过程如下:

![select]({{ '_posts/image/java/netty/select.png' | prepend: site.baseurl }})
1. 使用copy_from_user从用户空间拷贝fd_set到内核空间

2. 注册回调函数__pollwait

3. 遍历所有fd，调用其对应的poll方法（对于socket，这个poll方法是sock_poll，sock_poll根据情况会调用到tcp_poll,udp_poll或者datagram_poll）

4. 以tcp_poll为例，其核心实现就是__pollwait，也就是上面注册的回调函数。

5. __pollwait的主要工作就是把current（当前进程）挂到设备的等待队列中，不同的设备有不同的等待队列，对于tcp_poll来说，其等待队列是sk->sk_sleep（注意把进程挂到等待队列中并不代表进程已经睡眠了）。在设备收到一条消息（网络设备）或填写完文件数据（磁盘设备）后，会唤醒设备等待队列上睡眠的进程，这时current便被唤醒了。

6. poll方法返回时会返回一个描述读写操作是否就绪的mask掩码，根据这个mask掩码给fd_set赋值。

7. 如果遍历完所有的fd，还没有返回一个可读写的mask掩码，则会调用schedule_timeout是调用select的进程（也就是current）进入睡眠。当设备驱动发生自身资源可读写后，会唤醒其等待队列上睡眠的进程。如果超过一定的超时时间（schedule_timeout指定），还是没人唤醒，则调用select的进程会重新被唤醒获得CPU，进而重新遍历fd，判断有没有就绪的fd。

8. 把fd_set从内核空间拷贝到用户空间。

总结：

select的几大缺点：

1. 每次调用select，都需要把fd集合从用户态拷贝到内核态，这个开销在fd很多时会很大

2. 同时每次调用select都需要在内核遍历传递进来的所有fd，这个开销在fd很多时也很大

3. select支持的文件描述符数量太小了，默认是1024

* poll的实现

和select机制类似,本质上没有多大差别,管理多个描述符也是进行轮询,根据描述符的状态进行处理,但是poll不受最大文件描述符数量的限制.poll和select同样存在的一个缺点就是,包含大量文件描述符的数组被整体复制于用户态和内核的地址空间之间,而不论这些文件描述符是否就绪,开销随着文件描述符的增加而线性增大.

```c
#include <poll.h>
int poll (struct pollfd *fds, unsigned int nfds, int timeout);

// pollfd
struct pollfd {
    int fd;       // 文件描述符
    short events; // 等待的事件
    short revents;// 实际发生的事件
}
```

每一个pollfd结构体指定了一个被监视的文件描述符,可以传递多个结构体,指示poll()监视多个文件描述符.每个结构体的events域是监视该文件描述符的事件掩码,由用户设置,revents域是文件描述符的操作结果事件掩码,内核在调用返回时设置这个域.events域中请求的任何事件都可能个在reverts域中返回.合法事件如下:

​        POLLIN 　　　　　　　　有数据可读。

　　POLLRDNORM 　　　　  有普通数据可读。

　　POLLRDBAND　　　　　 有优先数据可读。

　　POLLPRI　　　　　　　　 有紧迫数据可读。

　　POLLOUT　　　　　　      写数据不会导致阻塞。

　　POLLWRNORM　　　　　  写普通数据不会导致阻塞。

　　POLLWRBAND　　　　　   写优先数据不会导致阻塞。

　　POLLMSGSIGPOLL 　　　　消息可用。

　　此外，revents域中还可能返回下列事件：
　　POLLER　　   指定的文件描述符发生错误。

　　POLLHUP　　 指定的文件描述符挂起事件。

　　POLLNVAL　　指定的文件描述符非法。

这些事件在events域中无意义，因为它们在合适的时候总是会从revents中返回。

　　使用poll()和select()不一样，你不需要显式地请求异常情况报告。
　　POLLIN | POLLPRI等价于select()的读事件，POLLOUT |POLLWRBAND等价于select()的写事件。POLLIN等价于POLLRDNORM |POLLRDBAND，而POLLOUT则等价于POLLWRNORM。例如，要同时监视一个文件描述符是否可读和可写，我们可以设置 events为POLLIN |POLLOUT。在poll返回时，我们可以检查revents中的标志，对应于文件描述符请求的events结构体。如果POLLIN事件被设置，则文件描述符可以被读取而不阻塞。如果POLLOUT被设置，则文件描述符可以写入而不导致阻塞。这些标志并不是互斥的：它们可能被同时设置，表示这个文件描述符的读取和写入操作都会正常返回而不阻塞。

　　timeout参数指定等待的毫秒数，无论I/O是否准备好，poll都会返回。timeout指定为负数值表示无限超时，使poll()一直挂起直到一个指定事件发生；timeout为0指示poll调用立即返回并列出准备好I/O的文件描述符，但并不等待其它的事件。这种情况下，poll()就像它的名字那样，一旦选举出来，立即返回。


　　返回值和错误代码
　　成功时，poll()返回结构体中revents域不为0的文件描述符个数；如果在超时前没有任何事件发生，poll()返回0；失败时，poll()返回-1，并设置errno为下列值之一：
　　EBADF　　       一个或多个结构体中指定的文件描述符无效。

　　EFAULTfds　　 指针指向的地址超出进程的地址空间。

　　EINTR　　　　  请求的事件之前产生一个信号，调用可以重新发起。

　　EINVALnfds　　参数超出PLIMIT_NOFILE值。

　　ENOMEM　　     可用内存不足，无法完成请求。

* epoll的实现

epoll既然是对select和poll的改进，就应该能避免上述的三个缺点。那epoll都是怎么解决的呢？在此之前，我们先看一下epoll和select和poll的调用接口上的不同，select和poll都只提供了一个函数——select或者poll函数。而epoll提供了三个函数，epoll_create,epoll_ctl和epoll_wait，epoll_create是创建一个epoll句柄；epoll_ctl是注册要监听的事件类型；epoll_wait则是等待事件的产生。

　　对于第一个缺点，epoll的解决方案在epoll_ctl函数中。每次注册新的事件到epoll句柄中时（在epoll_ctl中指定EPOLL_CTL_ADD），会把所有的fd拷贝进内核，而不是在epoll_wait的时候重复拷贝。epoll保证了每个fd在整个过程中只会拷贝一次。

　　对于第二个缺点，epoll的解决方案不像select或poll一样每次都把current轮流加入fd对应的设备等待队列中，而只在epoll_ctl时把current挂一遍（这一遍必不可少）并为每个fd指定一个回调函数，当设备就绪，唤醒等待队列上的等待者时，就会调用这个回调函数，而这个回调函数会把就绪的fd加入一个就绪链表）。epoll_wait的工作实际上就是在这个就绪链表中查看有没有就绪的fd（利用schedule_timeout()实现睡一会，判断一会的效果，和select实现中的第7步是类似的）。

　　对于第三个缺点，epoll没有这个限制，它所支持的FD上限是最大可以打开文件的数目，这个数字一般远大于2048,举个例子,在1GB内存的机器上大约是10万左右，具体数目可以cat /proc/sys/fs/file-max察看,一般来说这个数目和系统内存关系很大。

* 总结

select，poll实现需要自己不断轮询所有fd集合，直到设备就绪，期间可能要睡眠和唤醒多次交替。而epoll其实也需要调用epoll_wait不断轮询就绪链表，期间也可能多次睡眠和唤醒交替，但是它是设备就绪时，调用回调函数，把就绪fd放入就绪链表中，并唤醒在epoll_wait中进入睡眠的进程。虽然都要睡眠和交替，但是select和poll在“醒着”的时候要遍历整个fd集合，而epoll在“醒着”的时候只要判断一下就绪链表是否为空就行了，这节省了大量的CPU时间。这就是回调机制带来的性能提升。

select，poll每次调用都要把fd集合从用户态往内核态拷贝一次，并且要把current往设备等待队列中挂一次，而epoll只要一次拷贝，而且把current往等待队列上挂也只挂一次（在epoll_wait的开始，注意这里的等待队列并不是设备等待队列，只是一个epoll内部定义的等待队列）。这也能节省不少的开销。







参考

1. Netty权威指南
2. https://www.cnblogs.com/Anker/p/3265058.html

<hr>
​最后的最后，老婆我爱你。








